{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article extracted and saved for blackassign0001\n",
      "Article extracted and saved for blackassign0002\n",
      "Article extracted and saved for blackassign0003\n",
      "Article extracted and saved for blackassign0004\n",
      "Article extracted and saved for blackassign0005\n",
      "Article extracted and saved for blackassign0006\n",
      "Article extracted and saved for blackassign0007\n",
      "Article extracted and saved for blackassign0008\n",
      "Article extracted and saved for blackassign0009\n",
      "Article extracted and saved for blackassign0010\n",
      "Article extracted and saved for blackassign0011\n",
      "Article extracted and saved for blackassign0012\n",
      "Article extracted and saved for blackassign0013\n",
      "Article extracted and saved for blackassign0014\n",
      "Article extracted and saved for blackassign0015\n",
      "Article extracted and saved for blackassign0016\n",
      "Article extracted and saved for blackassign0017\n",
      "Article extracted and saved for blackassign0018\n",
      "Article extracted and saved for blackassign0019\n",
      "Article extracted and saved for blackassign0020\n",
      "Article extracted and saved for blackassign0021\n",
      "Article extracted and saved for blackassign0022\n",
      "Article extracted and saved for blackassign0023\n",
      "Article extracted and saved for blackassign0024\n",
      "Article extracted and saved for blackassign0025\n",
      "Article extracted and saved for blackassign0026\n",
      "Article extracted and saved for blackassign0027\n",
      "Article extracted and saved for blackassign0028\n",
      "Article extracted and saved for blackassign0029\n",
      "Article extracted and saved for blackassign0030\n",
      "Article extracted and saved for blackassign0031\n",
      "Article extracted and saved for blackassign0032\n",
      "Article extracted and saved for blackassign0033\n",
      "Article extracted and saved for blackassign0034\n",
      "Article extracted and saved for blackassign0035\n",
      "Failed to extract article for blackassign0036. Error: RequestException occurred while fetching https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Article extracted and saved for blackassign0037\n",
      "Article extracted and saved for blackassign0038\n",
      "Article extracted and saved for blackassign0039\n",
      "Article extracted and saved for blackassign0040\n",
      "Article extracted and saved for blackassign0041\n",
      "Article extracted and saved for blackassign0042\n",
      "Article extracted and saved for blackassign0043\n",
      "Article extracted and saved for blackassign0044\n",
      "Article extracted and saved for blackassign0045\n",
      "Article extracted and saved for blackassign0046\n",
      "Article extracted and saved for blackassign0047\n",
      "Article extracted and saved for blackassign0048\n",
      "Failed to extract article for blackassign0049. Error: RequestException occurred while fetching https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Article extracted and saved for blackassign0050\n",
      "Article extracted and saved for blackassign0051\n",
      "Article extracted and saved for blackassign0052\n",
      "Article extracted and saved for blackassign0053\n",
      "Article extracted and saved for blackassign0054\n",
      "Article extracted and saved for blackassign0055\n",
      "Article extracted and saved for blackassign0056\n",
      "Article extracted and saved for blackassign0057\n",
      "Article extracted and saved for blackassign0058\n",
      "Article extracted and saved for blackassign0059\n",
      "Article extracted and saved for blackassign0060\n",
      "Article extracted and saved for blackassign0061\n",
      "Article extracted and saved for blackassign0062\n",
      "Article extracted and saved for blackassign0063\n",
      "Article extracted and saved for blackassign0064\n",
      "Article extracted and saved for blackassign0065\n",
      "Article extracted and saved for blackassign0066\n",
      "Article extracted and saved for blackassign0067\n",
      "Article extracted and saved for blackassign0068\n",
      "Article extracted and saved for blackassign0069\n",
      "Article extracted and saved for blackassign0070\n",
      "Article extracted and saved for blackassign0071\n",
      "Article extracted and saved for blackassign0072\n",
      "Article extracted and saved for blackassign0073\n",
      "Article extracted and saved for blackassign0074\n",
      "Article extracted and saved for blackassign0075\n",
      "Article extracted and saved for blackassign0076\n",
      "Article extracted and saved for blackassign0077\n",
      "Article extracted and saved for blackassign0078\n",
      "Article extracted and saved for blackassign0079\n",
      "Article extracted and saved for blackassign0080\n",
      "Article extracted and saved for blackassign0081\n",
      "Article extracted and saved for blackassign0082\n",
      "Article extracted and saved for blackassign0083\n",
      "Article extracted and saved for blackassign0084\n",
      "Article extracted and saved for blackassign0085\n",
      "Article extracted and saved for blackassign0086\n",
      "Article extracted and saved for blackassign0087\n",
      "Article extracted and saved for blackassign0088\n",
      "Article extracted and saved for blackassign0089\n",
      "Article extracted and saved for blackassign0090\n",
      "Article extracted and saved for blackassign0091\n",
      "Article extracted and saved for blackassign0092\n",
      "Article extracted and saved for blackassign0093\n",
      "Article extracted and saved for blackassign0094\n",
      "Article extracted and saved for blackassign0095\n",
      "Article extracted and saved for blackassign0096\n",
      "Article extracted and saved for blackassign0097\n",
      "Article extracted and saved for blackassign0098\n",
      "Article extracted and saved for blackassign0099\n",
      "Article extracted and saved for blackassign0100\n",
      "Extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from readability import Document\n",
    "import os\n",
    "\n",
    "# Function to extract article text from URL\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        # Fetch webpage content\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        \n",
    "        # Parse webpage content with readability\n",
    "        doc = Document(response.text)\n",
    "        article_title = doc.short_title()\n",
    "        article_text = doc.summary()\n",
    "        \n",
    "        return article_title, article_text\n",
    "    except requests.RequestException as e:\n",
    "        return None, f\"RequestException occurred while fetching {url}: {e}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Error occurred while processing {url}: {e}\"\n",
    "\n",
    "# Read the URLs and URL IDs from the Excel file\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# Create a directory to store the text files\n",
    "if not os.path.exists('article_texts'):\n",
    "    os.makedirs('article_texts')\n",
    "\n",
    "# Iterate through URLs\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Extract article text\n",
    "    article_title, article_text = extract_article_text(url)\n",
    "    \n",
    "    if article_title and article_text:\n",
    "        # Save extracted article into a text file\n",
    "        with open(f'article_texts/{url_id}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(f'Title: {article_title}\\n\\n')\n",
    "            f.write(article_text)\n",
    "            \n",
    "        print(f\"Article extracted and saved for {url_id}\")\n",
    "    else:\n",
    "        print(f\"Failed to extract article for {url_id}. Error: {article_text}\")\n",
    "\n",
    "print(\"Extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
